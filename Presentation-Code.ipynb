{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Run Neural Nets on GPUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  189    0 0xffffff7f82fdb000 0x2000     0x2000     com.nvidia.CUDA (1.1.0) <4 1>\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "kextstat | grep -i cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Commands to Run Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: Could not find or load main class org.deeplearning4j.gpu.examples.MnistExample\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "time java -cp dl4j_examples/target/org.deeplearning-1.0-SNAPSHOT.jar org.deeplearning4j.gpu.examples.MnistExample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... load and setup data\n",
      "... loading data\n",
      "... building the model\n",
      "... training the model\n",
      "epoch 1, minibatch 500/500, validation error 9.340000 %\n",
      "epoch 1, minibatch 500/500, test error of best model 9.590000 %\n",
      "epoch 2, minibatch 500/500, validation error 8.540000 %\n",
      "epoch 2, minibatch 500/500, test error of best model 8.810000 %\n",
      "epoch 3, minibatch 500/500, validation error 8.320000 %\n",
      "epoch 3, minibatch 500/500, test error of best model 8.530000 %\n",
      "epoch 4, minibatch 500/500, validation error 8.260000 %\n",
      "epoch 4, minibatch 500/500, test error of best model 8.310000 %\n",
      "epoch 5, minibatch 500/500, validation error 8.050000 %\n",
      "epoch 5, minibatch 500/500, test error of best model 8.210000 %\n",
      "epoch 6, minibatch 500/500, validation error 7.980000 %\n",
      "epoch 6, minibatch 500/500, test error of best model 8.160000 %\n",
      "epoch 7, minibatch 500/500, validation error 7.830000 %\n",
      "epoch 7, minibatch 500/500, test error of best model 8.000000 %\n",
      "epoch 8, minibatch 500/500, validation error 7.780000 %\n",
      "epoch 8, minibatch 500/500, test error of best model 7.870000 %\n",
      "epoch 9, minibatch 500/500, validation error 7.710000 %\n",
      "epoch 9, minibatch 500/500, test error of best model 7.860000 %\n",
      "epoch 10, minibatch 500/500, validation error 7.710000 %\n",
      "epoch 11, minibatch 500/500, validation error 7.640000 %\n",
      "epoch 11, minibatch 500/500, test error of best model 7.760000 %\n",
      "epoch 12, minibatch 500/500, validation error 7.580000 %\n",
      "epoch 12, minibatch 500/500, test error of best model 7.760000 %\n",
      "epoch 13, minibatch 500/500, validation error 7.550000 %\n",
      "epoch 13, minibatch 500/500, test error of best model 7.730000 %\n",
      "epoch 14, minibatch 500/500, validation error 7.470000 %\n",
      "epoch 14, minibatch 500/500, test error of best model 7.760000 %\n",
      "epoch 15, minibatch 500/500, validation error 7.390000 %\n",
      "epoch 15, minibatch 500/500, test error of best model 7.740000 %\n",
      "epoch 16, minibatch 500/500, validation error 7.330000 %\n",
      "epoch 16, minibatch 500/500, test error of best model 7.760000 %\n",
      "epoch 17, minibatch 500/500, validation error 7.320000 %\n",
      "epoch 17, minibatch 500/500, test error of best model 7.800000 %\n",
      "epoch 18, minibatch 500/500, validation error 7.290000 %\n",
      "epoch 18, minibatch 500/500, test error of best model 7.770000 %\n",
      "epoch 19, minibatch 500/500, validation error 7.260000 %\n",
      "epoch 19, minibatch 500/500, test error of best model 7.790000 %\n",
      "epoch 20, minibatch 500/500, validation error 7.200000 %\n",
      "epoch 20, minibatch 500/500, test error of best model 7.800000 %\n",
      "epoch 21, minibatch 500/500, validation error 7.190000 %\n",
      "epoch 21, minibatch 500/500, test error of best model 7.790000 %\n",
      "epoch 22, minibatch 500/500, validation error 7.150000 %\n",
      "epoch 22, minibatch 500/500, test error of best model 7.750000 %\n",
      "epoch 23, minibatch 500/500, validation error 7.160000 %\n",
      "epoch 24, minibatch 500/500, validation error 7.140000 %\n",
      "epoch 24, minibatch 500/500, test error of best model 7.710000 %\n",
      "epoch 25, minibatch 500/500, validation error 7.110000 %\n",
      "epoch 25, minibatch 500/500, test error of best model 7.690000 %\n",
      "epoch 26, minibatch 500/500, validation error 7.130000 %\n",
      "epoch 27, minibatch 500/500, validation error 7.080000 %\n",
      "epoch 27, minibatch 500/500, test error of best model 7.670000 %\n",
      "epoch 28, minibatch 500/500, validation error 7.100000 %\n",
      "epoch 29, minibatch 500/500, validation error 7.070000 %\n",
      "epoch 29, minibatch 500/500, test error of best model 7.650000 %\n",
      "epoch 30, minibatch 500/500, validation error 7.040000 %\n",
      "epoch 30, minibatch 500/500, test error of best model 7.660000 %\n",
      "epoch 31, minibatch 500/500, validation error 7.040000 %\n",
      "epoch 32, minibatch 500/500, validation error 7.010000 %\n",
      "epoch 32, minibatch 500/500, test error of best model 7.650000 %\n",
      "epoch 33, minibatch 500/500, validation error 7.000000 %\n",
      "epoch 33, minibatch 500/500, test error of best model 7.640000 %\n",
      "epoch 34, minibatch 500/500, validation error 7.020000 %\n",
      "epoch 35, minibatch 500/500, validation error 7.010000 %\n",
      "epoch 36, minibatch 500/500, validation error 6.990000 %\n",
      "epoch 36, minibatch 500/500, test error of best model 7.600000 %\n",
      "epoch 37, minibatch 500/500, validation error 6.960000 %\n",
      "epoch 37, minibatch 500/500, test error of best model 7.630000 %\n",
      "epoch 38, minibatch 500/500, validation error 6.950000 %\n",
      "epoch 38, minibatch 500/500, test error of best model 7.610000 %\n",
      "epoch 39, minibatch 500/500, validation error 6.940000 %\n",
      "epoch 39, minibatch 500/500, test error of best model 7.590000 %\n",
      "epoch 40, minibatch 500/500, validation error 6.900000 %\n",
      "epoch 40, minibatch 500/500, test error of best model 7.580000 %\n",
      "epoch 41, minibatch 500/500, validation error 6.890000 %\n",
      "epoch 41, minibatch 500/500, test error of best model 7.570000 %\n",
      "epoch 42, minibatch 500/500, validation error 6.950000 %\n",
      "epoch 43, minibatch 500/500, validation error 6.950000 %\n",
      "epoch 44, minibatch 500/500, validation error 6.950000 %\n",
      "epoch 45, minibatch 500/500, validation error 6.940000 %\n",
      "epoch 46, minibatch 500/500, validation error 6.940000 %\n",
      "epoch 47, minibatch 500/500, validation error 6.930000 %\n",
      "epoch 48, minibatch 500/500, validation error 6.910000 %\n",
      "epoch 49, minibatch 500/500, validation error 6.940000 %\n",
      "epoch 50, minibatch 500/500, validation error 6.940000 %\n",
      "epoch 51, minibatch 500/500, validation error 6.940000 %\n",
      "epoch 52, minibatch 500/500, validation error 6.950000 %\n",
      "epoch 53, minibatch 500/500, validation error 6.980000 %\n",
      "epoch 54, minibatch 500/500, validation error 6.970000 %\n",
      "epoch 55, minibatch 500/500, validation error 6.970000 %\n",
      "epoch 56, minibatch 500/500, validation error 6.950000 %\n",
      "epoch 57, minibatch 500/500, validation error 6.930000 %\n",
      "epoch 58, minibatch 500/500, validation error 6.920000 %\n",
      "epoch 59, minibatch 500/500, validation error 6.910000 %\n",
      "epoch 60, minibatch 500/500, validation error 6.930000 %\n",
      "epoch 61, minibatch 500/500, validation error 6.920000 %\n",
      "epoch 62, minibatch 500/500, validation error 6.930000 %\n",
      "epoch 63, minibatch 500/500, validation error 6.970000 %\n",
      "epoch 64, minibatch 500/500, validation error 6.970000 %\n",
      "epoch 65, minibatch 500/500, validation error 6.960000 %\n",
      "epoch 66, minibatch 500/500, validation error 6.970000 %\n",
      "epoch 67, minibatch 500/500, validation error 6.950000 %\n",
      "epoch 68, minibatch 500/500, validation error 6.940000 %\n",
      "epoch 69, minibatch 500/500, validation error 6.950000 %\n",
      "epoch 70, minibatch 500/500, validation error 6.950000 %\n",
      "epoch 71, minibatch 500/500, validation error 6.950000 %\n",
      "epoch 72, minibatch 500/500, validation error 6.960000 %\n",
      "epoch 73, minibatch 500/500, validation error 6.970000 %\n",
      "epoch 74, minibatch 500/500, validation error 6.970000 %\n",
      "epoch 75, minibatch 500/500, validation error 6.970000 %\n",
      "epoch 76, minibatch 500/500, validation error 6.960000 %\n",
      "epoch 77, minibatch 500/500, validation error 6.970000 %\n",
      "epoch 78, minibatch 500/500, validation error 6.980000 %\n",
      "epoch 79, minibatch 500/500, validation error 6.980000 %\n",
      "Optimization complete with best validation score of 6.890000 %, with test performance 7.570000 %\n",
      "The code run for 80 epochs, with 1.941737 epochs/sec\n",
      "... evaluating the model\n",
      "Model accuracy:  0.9245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GT 750M\n",
      "The code for file __file__ ran for 41.2s\n",
      "\n",
      "real\t0m47.290s\n",
      "user\t0m38.344s\n",
      "sys\t0m5.767s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "time THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python theano_examples/theano_example.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0726 17:38:08.281308 2052363008 caffe.cpp:113] Use GPU with device ID 0\n",
      "I0726 17:38:08.518748 2052363008 caffe.cpp:121] Starting Optimization\n",
      "I0726 17:38:08.518784 2052363008 solver.cpp:32] Initializing solver from parameters: \n",
      "test_iter: 100\n",
      "test_interval: 500\n",
      "base_lr: 0.01\n",
      "display: 100\n",
      "max_iter: 10000\n",
      "lr_policy: \"inv\"\n",
      "gamma: 0.0001\n",
      "power: 0.75\n",
      "momentum: 0.9\n",
      "weight_decay: 0.0005\n",
      "snapshot: 5000\n",
      "snapshot_prefix: \"lenet\"\n",
      "solver_mode: GPU\n",
      "net: \"lenet_train_test.prototxt\"\n",
      "I0726 17:38:08.518860 2052363008 solver.cpp:70] Creating training net from net file: lenet_train_test.prototxt\n",
      "I0726 17:38:08.519075 2052363008 net.cpp:287] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist\n",
      "I0726 17:38:08.519101 2052363008 net.cpp:287] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy\n",
      "I0726 17:38:08.519109 2052363008 net.cpp:42] Initializing net from parameters: \n",
      "name: \"LeNet\"\n",
      "state {\n",
      "  phase: TRAIN\n",
      "}\n",
      "layer {\n",
      "  name: \"mnist\"\n",
      "  type: \"Data\"\n",
      "  top: \"data\"\n",
      "  top: \"label\"\n",
      "  include {\n",
      "    phase: TRAIN\n",
      "  }\n",
      "  transform_param {\n",
      "    scale: 0.00390625\n",
      "  }\n",
      "  data_param {\n",
      "    source: \"../data/mnist/mnist_train_lmdb\"\n",
      "    batch_size: 64\n",
      "    backend: LMDB\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 20\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv2\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"conv2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 50\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool2\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"pool2\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"ip1\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool2\"\n",
      "  top: \"ip1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 500\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"ip1\"\n",
      "  top: \"ip1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"ip2\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"ip1\"\n",
      "  top: \"ip2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 10\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"loss\"\n",
      "  type: \"SoftmaxWithLoss\"\n",
      "  bottom: \"ip2\"\n",
      "  bottom: \"label\"\n",
      "  top: \"loss\"\n",
      "}\n",
      "I0726 17:38:08.519239 2052363008 layer_factory.hpp:74] Creating layer mnist\n",
      "I0726 17:38:08.519254 2052363008 net.cpp:90] Creating Layer mnist\n",
      "I0726 17:38:08.519263 2052363008 net.cpp:368] mnist -> data\n",
      "I0726 17:38:08.519281 2052363008 net.cpp:368] mnist -> label\n",
      "I0726 17:38:08.519289 2052363008 net.cpp:120] Setting up mnist\n",
      "I0726 17:38:08.519425 2052363008 db_lmdb.cpp:22] Opened lmdb ../data/mnist/mnist_train_lmdb\n",
      "I0726 17:38:08.519482 2052363008 data_layer.cpp:52] output data size: 64,1,28,28\n",
      "I0726 17:38:08.519564 2052363008 net.cpp:127] Top shape: 64 1 28 28 (50176)\n",
      "I0726 17:38:08.519589 2052363008 net.cpp:127] Top shape: 64 (64)\n",
      "I0726 17:38:08.519605 2052363008 layer_factory.hpp:74] Creating layer conv1\n",
      "I0726 17:38:08.519616 2052363008 net.cpp:90] Creating Layer conv1\n",
      "I0726 17:38:08.519621 2052363008 net.cpp:410] conv1 <- data\n",
      "I0726 17:38:08.519629 2052363008 net.cpp:368] conv1 -> conv1\n",
      "I0726 17:38:08.519636 2052363008 net.cpp:120] Setting up conv1\n",
      "I0726 17:38:08.574127 2052363008 net.cpp:127] Top shape: 64 20 24 24 (737280)\n",
      "I0726 17:38:08.574170 2052363008 layer_factory.hpp:74] Creating layer pool1\n",
      "I0726 17:38:08.574183 2052363008 net.cpp:90] Creating Layer pool1\n",
      "I0726 17:38:08.574188 2052363008 net.cpp:410] pool1 <- conv1\n",
      "I0726 17:38:08.574195 2052363008 net.cpp:368] pool1 -> pool1\n",
      "I0726 17:38:08.574203 2052363008 net.cpp:120] Setting up pool1\n",
      "I0726 17:38:08.574409 2052363008 net.cpp:127] Top shape: 64 20 12 12 (184320)\n",
      "I0726 17:38:08.574419 2052363008 layer_factory.hpp:74] Creating layer conv2\n",
      "I0726 17:38:08.574440 2052363008 net.cpp:90] Creating Layer conv2\n",
      "I0726 17:38:08.574443 2052363008 net.cpp:410] conv2 <- pool1\n",
      "I0726 17:38:08.574450 2052363008 net.cpp:368] conv2 -> conv2\n",
      "I0726 17:38:08.574457 2052363008 net.cpp:120] Setting up conv2\n",
      "I0726 17:38:08.574879 2052363008 net.cpp:127] Top shape: 64 50 8 8 (204800)\n",
      "I0726 17:38:08.574892 2052363008 layer_factory.hpp:74] Creating layer pool2\n",
      "I0726 17:38:08.574913 2052363008 net.cpp:90] Creating Layer pool2\n",
      "I0726 17:38:08.574916 2052363008 net.cpp:410] pool2 <- conv2\n",
      "I0726 17:38:08.574923 2052363008 net.cpp:368] pool2 -> pool2\n",
      "I0726 17:38:08.574928 2052363008 net.cpp:120] Setting up pool2\n",
      "I0726 17:38:08.574985 2052363008 net.cpp:127] Top shape: 64 50 4 4 (51200)\n",
      "I0726 17:38:08.574991 2052363008 layer_factory.hpp:74] Creating layer ip1\n",
      "I0726 17:38:08.575009 2052363008 net.cpp:90] Creating Layer ip1\n",
      "I0726 17:38:08.575013 2052363008 net.cpp:410] ip1 <- pool2\n",
      "I0726 17:38:08.575021 2052363008 net.cpp:368] ip1 -> ip1\n",
      "I0726 17:38:08.575026 2052363008 net.cpp:120] Setting up ip1\n",
      "I0726 17:38:08.577965 2052363008 net.cpp:127] Top shape: 64 500 (32000)\n",
      "I0726 17:38:08.577993 2052363008 layer_factory.hpp:74] Creating layer relu1\n",
      "I0726 17:38:08.578007 2052363008 net.cpp:90] Creating Layer relu1\n",
      "I0726 17:38:08.578012 2052363008 net.cpp:410] relu1 <- ip1\n",
      "I0726 17:38:08.578017 2052363008 net.cpp:357] relu1 -> ip1 (in-place)\n",
      "I0726 17:38:08.578024 2052363008 net.cpp:120] Setting up relu1\n",
      "I0726 17:38:08.578104 2052363008 net.cpp:127] Top shape: 64 500 (32000)\n",
      "I0726 17:38:08.578110 2052363008 layer_factory.hpp:74] Creating layer ip2\n",
      "I0726 17:38:08.578129 2052363008 net.cpp:90] Creating Layer ip2\n",
      "I0726 17:38:08.578132 2052363008 net.cpp:410] ip2 <- ip1\n",
      "I0726 17:38:08.578140 2052363008 net.cpp:368] ip2 -> ip2\n",
      "I0726 17:38:08.578146 2052363008 net.cpp:120] Setting up ip2\n",
      "I0726 17:38:08.578212 2052363008 net.cpp:127] Top shape: 64 10 (640)\n",
      "I0726 17:38:08.578219 2052363008 layer_factory.hpp:74] Creating layer loss\n",
      "I0726 17:38:08.578238 2052363008 net.cpp:90] Creating Layer loss\n",
      "I0726 17:38:08.578243 2052363008 net.cpp:410] loss <- ip2\n",
      "I0726 17:38:08.578248 2052363008 net.cpp:410] loss <- label\n",
      "I0726 17:38:08.578253 2052363008 net.cpp:368] loss -> loss\n",
      "I0726 17:38:08.578260 2052363008 net.cpp:120] Setting up loss\n",
      "I0726 17:38:08.578266 2052363008 layer_factory.hpp:74] Creating layer loss\n",
      "I0726 17:38:08.578446 2052363008 net.cpp:127] Top shape: (1)\n",
      "I0726 17:38:08.578456 2052363008 net.cpp:129]     with loss weight 1\n",
      "I0726 17:38:08.578480 2052363008 net.cpp:192] loss needs backward computation.\n",
      "I0726 17:38:08.578485 2052363008 net.cpp:192] ip2 needs backward computation.\n",
      "I0726 17:38:08.578490 2052363008 net.cpp:192] relu1 needs backward computation.\n",
      "I0726 17:38:08.578493 2052363008 net.cpp:192] ip1 needs backward computation.\n",
      "I0726 17:38:08.578496 2052363008 net.cpp:192] pool2 needs backward computation.\n",
      "I0726 17:38:08.578500 2052363008 net.cpp:192] conv2 needs backward computation.\n",
      "I0726 17:38:08.578505 2052363008 net.cpp:192] pool1 needs backward computation.\n",
      "I0726 17:38:08.578508 2052363008 net.cpp:192] conv1 needs backward computation.\n",
      "I0726 17:38:08.578512 2052363008 net.cpp:194] mnist does not need backward computation.\n",
      "I0726 17:38:08.578516 2052363008 net.cpp:235] This network produces output loss\n",
      "I0726 17:38:08.578526 2052363008 net.cpp:482] Collecting Learning Rate and Weight Decay.\n",
      "I0726 17:38:08.578534 2052363008 net.cpp:247] Network initialization done.\n",
      "I0726 17:38:08.578538 2052363008 net.cpp:248] Memory required for data: 5169924\n",
      "I0726 17:38:08.578774 2052363008 solver.cpp:154] Creating test net (#0) specified by net file: lenet_train_test.prototxt\n",
      "I0726 17:38:08.578807 2052363008 net.cpp:287] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist\n",
      "I0726 17:38:08.578817 2052363008 net.cpp:42] Initializing net from parameters: \n",
      "name: \"LeNet\"\n",
      "state {\n",
      "  phase: TEST\n",
      "}\n",
      "layer {\n",
      "  name: \"mnist\"\n",
      "  type: \"Data\"\n",
      "  top: \"data\"\n",
      "  top: \"label\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "  transform_param {\n",
      "    scale: 0.00390625\n",
      "  }\n",
      "  data_param {\n",
      "    source: \"../data/mnist/mnist_test_lmdb\"\n",
      "    batch_size: 100\n",
      "    backend: LMDB\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 20\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv2\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"conv2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 50\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool2\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"pool2\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"ip1\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool2\"\n",
      "  top: \"ip1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 500\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"ip1\"\n",
      "  top: \"ip1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"ip2\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"ip1\"\n",
      "  top: \"ip2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 10\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"accuracy\"\n",
      "  type: \"Accuracy\"\n",
      "  bottom: \"ip2\"\n",
      "  bottom: \"label\"\n",
      "  top: \"accuracy\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"loss\"\n",
      "  type: \"SoftmaxWithLoss\"\n",
      "  bottom: \"ip2\"\n",
      "  bottom: \"label\"\n",
      "  top: \"loss\"\n",
      "}\n",
      "I0726 17:38:08.578994 2052363008 layer_factory.hpp:74] Creating layer mnist\n",
      "I0726 17:38:08.579013 2052363008 net.cpp:90] Creating Layer mnist\n",
      "I0726 17:38:08.579028 2052363008 net.cpp:368] mnist -> data\n",
      "I0726 17:38:08.579036 2052363008 net.cpp:368] mnist -> label\n",
      "I0726 17:38:08.579042 2052363008 net.cpp:120] Setting up mnist\n",
      "I0726 17:38:08.579171 2052363008 db_lmdb.cpp:22] Opened lmdb ../data/mnist/mnist_test_lmdb\n",
      "I0726 17:38:08.579201 2052363008 data_layer.cpp:52] output data size: 100,1,28,28\n",
      "I0726 17:38:08.579255 2052363008 net.cpp:127] Top shape: 100 1 28 28 (78400)\n",
      "I0726 17:38:08.579272 2052363008 net.cpp:127] Top shape: 100 (100)\n",
      "I0726 17:38:08.579288 2052363008 layer_factory.hpp:74] Creating layer label_mnist_1_split\n",
      "I0726 17:38:08.579298 2052363008 net.cpp:90] Creating Layer label_mnist_1_split\n",
      "I0726 17:38:08.579303 2052363008 net.cpp:410] label_mnist_1_split <- label\n",
      "I0726 17:38:08.579308 2052363008 net.cpp:368] label_mnist_1_split -> label_mnist_1_split_0\n",
      "I0726 17:38:08.579315 2052363008 net.cpp:368] label_mnist_1_split -> label_mnist_1_split_1\n",
      "I0726 17:38:08.579321 2052363008 net.cpp:120] Setting up label_mnist_1_split\n",
      "I0726 17:38:08.579326 2052363008 net.cpp:127] Top shape: 100 (100)\n",
      "I0726 17:38:08.579331 2052363008 net.cpp:127] Top shape: 100 (100)\n",
      "I0726 17:38:08.579336 2052363008 layer_factory.hpp:74] Creating layer conv1\n",
      "I0726 17:38:08.579342 2052363008 net.cpp:90] Creating Layer conv1\n",
      "I0726 17:38:08.579346 2052363008 net.cpp:410] conv1 <- data\n",
      "I0726 17:38:08.579352 2052363008 net.cpp:368] conv1 -> conv1\n",
      "I0726 17:38:08.579360 2052363008 net.cpp:120] Setting up conv1\n",
      "I0726 17:38:08.579612 2052363008 net.cpp:127] Top shape: 100 20 24 24 (1152000)\n",
      "I0726 17:38:08.579635 2052363008 layer_factory.hpp:74] Creating layer pool1\n",
      "I0726 17:38:08.579643 2052363008 net.cpp:90] Creating Layer pool1\n",
      "I0726 17:38:08.579646 2052363008 net.cpp:410] pool1 <- conv1\n",
      "I0726 17:38:08.579651 2052363008 net.cpp:368] pool1 -> pool1\n",
      "I0726 17:38:08.579658 2052363008 net.cpp:120] Setting up pool1\n",
      "I0726 17:38:08.579712 2052363008 net.cpp:127] Top shape: 100 20 12 12 (288000)\n",
      "I0726 17:38:08.579731 2052363008 layer_factory.hpp:74] Creating layer conv2\n",
      "I0726 17:38:08.579752 2052363008 net.cpp:90] Creating Layer conv2\n",
      "I0726 17:38:08.579756 2052363008 net.cpp:410] conv2 <- pool1\n",
      "I0726 17:38:08.579762 2052363008 net.cpp:368] conv2 -> conv2\n",
      "I0726 17:38:08.579769 2052363008 net.cpp:120] Setting up conv2\n",
      "I0726 17:38:08.580221 2052363008 net.cpp:127] Top shape: 100 50 8 8 (320000)\n",
      "I0726 17:38:08.580247 2052363008 layer_factory.hpp:74] Creating layer pool2\n",
      "I0726 17:38:08.580253 2052363008 net.cpp:90] Creating Layer pool2\n",
      "I0726 17:38:08.580257 2052363008 net.cpp:410] pool2 <- conv2\n",
      "I0726 17:38:08.580262 2052363008 net.cpp:368] pool2 -> pool2\n",
      "I0726 17:38:08.580268 2052363008 net.cpp:120] Setting up pool2\n",
      "I0726 17:38:08.580324 2052363008 net.cpp:127] Top shape: 100 50 4 4 (80000)\n",
      "I0726 17:38:08.580330 2052363008 layer_factory.hpp:74] Creating layer ip1\n",
      "I0726 17:38:08.580348 2052363008 net.cpp:90] Creating Layer ip1\n",
      "I0726 17:38:08.580351 2052363008 net.cpp:410] ip1 <- pool2\n",
      "I0726 17:38:08.580358 2052363008 net.cpp:368] ip1 -> ip1\n",
      "I0726 17:38:08.580365 2052363008 net.cpp:120] Setting up ip1\n",
      "I0726 17:38:08.583117 2052363008 net.cpp:127] Top shape: 100 500 (50000)\n",
      "I0726 17:38:08.583129 2052363008 layer_factory.hpp:74] Creating layer relu1\n",
      "I0726 17:38:08.583145 2052363008 net.cpp:90] Creating Layer relu1\n",
      "I0726 17:38:08.583149 2052363008 net.cpp:410] relu1 <- ip1\n",
      "I0726 17:38:08.583154 2052363008 net.cpp:357] relu1 -> ip1 (in-place)\n",
      "I0726 17:38:08.583186 2052363008 net.cpp:120] Setting up relu1\n",
      "I0726 17:38:08.583263 2052363008 net.cpp:127] Top shape: 100 500 (50000)\n",
      "I0726 17:38:08.583271 2052363008 layer_factory.hpp:74] Creating layer ip2\n",
      "I0726 17:38:08.583291 2052363008 net.cpp:90] Creating Layer ip2\n",
      "I0726 17:38:08.583295 2052363008 net.cpp:410] ip2 <- ip1\n",
      "I0726 17:38:08.583307 2052363008 net.cpp:368] ip2 -> ip2\n",
      "I0726 17:38:08.583314 2052363008 net.cpp:120] Setting up ip2\n",
      "I0726 17:38:08.583370 2052363008 net.cpp:127] Top shape: 100 10 (1000)\n",
      "I0726 17:38:08.583377 2052363008 layer_factory.hpp:74] Creating layer ip2_ip2_0_split\n",
      "I0726 17:38:08.583395 2052363008 net.cpp:90] Creating Layer ip2_ip2_0_split\n",
      "I0726 17:38:08.583398 2052363008 net.cpp:410] ip2_ip2_0_split <- ip2\n",
      "I0726 17:38:08.583402 2052363008 net.cpp:368] ip2_ip2_0_split -> ip2_ip2_0_split_0\n",
      "I0726 17:38:08.583410 2052363008 net.cpp:368] ip2_ip2_0_split -> ip2_ip2_0_split_1\n",
      "I0726 17:38:08.583415 2052363008 net.cpp:120] Setting up ip2_ip2_0_split\n",
      "I0726 17:38:08.583420 2052363008 net.cpp:127] Top shape: 100 10 (1000)\n",
      "I0726 17:38:08.583425 2052363008 net.cpp:127] Top shape: 100 10 (1000)\n",
      "I0726 17:38:08.583428 2052363008 layer_factory.hpp:74] Creating layer accuracy\n",
      "I0726 17:38:08.583437 2052363008 net.cpp:90] Creating Layer accuracy\n",
      "I0726 17:38:08.583442 2052363008 net.cpp:410] accuracy <- ip2_ip2_0_split_0\n",
      "I0726 17:38:08.583446 2052363008 net.cpp:410] accuracy <- label_mnist_1_split_0\n",
      "I0726 17:38:08.583453 2052363008 net.cpp:368] accuracy -> accuracy\n",
      "I0726 17:38:08.583461 2052363008 net.cpp:120] Setting up accuracy\n",
      "I0726 17:38:08.583464 2052363008 net.cpp:127] Top shape: (1)\n",
      "I0726 17:38:08.583469 2052363008 layer_factory.hpp:74] Creating layer loss\n",
      "I0726 17:38:08.583474 2052363008 net.cpp:90] Creating Layer loss\n",
      "I0726 17:38:08.583478 2052363008 net.cpp:410] loss <- ip2_ip2_0_split_1\n",
      "I0726 17:38:08.583482 2052363008 net.cpp:410] loss <- label_mnist_1_split_1\n",
      "I0726 17:38:08.583487 2052363008 net.cpp:368] loss -> loss\n",
      "I0726 17:38:08.583492 2052363008 net.cpp:120] Setting up loss\n",
      "I0726 17:38:08.583498 2052363008 layer_factory.hpp:74] Creating layer loss\n",
      "I0726 17:38:08.584067 2052363008 net.cpp:127] Top shape: (1)\n",
      "I0726 17:38:08.584079 2052363008 net.cpp:129]     with loss weight 1\n",
      "I0726 17:38:08.584096 2052363008 net.cpp:192] loss needs backward computation.\n",
      "I0726 17:38:08.584103 2052363008 net.cpp:194] accuracy does not need backward computation.\n",
      "I0726 17:38:08.584107 2052363008 net.cpp:192] ip2_ip2_0_split needs backward computation.\n",
      "I0726 17:38:08.584111 2052363008 net.cpp:192] ip2 needs backward computation.\n",
      "I0726 17:38:08.584115 2052363008 net.cpp:192] relu1 needs backward computation.\n",
      "I0726 17:38:08.584146 2052363008 net.cpp:192] ip1 needs backward computation.\n",
      "I0726 17:38:08.584151 2052363008 net.cpp:192] pool2 needs backward computation.\n",
      "I0726 17:38:08.584166 2052363008 net.cpp:192] conv2 needs backward computation.\n",
      "I0726 17:38:08.584168 2052363008 net.cpp:192] pool1 needs backward computation.\n",
      "I0726 17:38:08.584172 2052363008 net.cpp:192] conv1 needs backward computation.\n",
      "I0726 17:38:08.584177 2052363008 net.cpp:194] label_mnist_1_split does not need backward computation.\n",
      "I0726 17:38:08.584180 2052363008 net.cpp:194] mnist does not need backward computation.\n",
      "I0726 17:38:08.584184 2052363008 net.cpp:235] This network produces output accuracy\n",
      "I0726 17:38:08.584188 2052363008 net.cpp:235] This network produces output loss\n",
      "I0726 17:38:08.584197 2052363008 net.cpp:482] Collecting Learning Rate and Weight Decay.\n",
      "I0726 17:38:08.584203 2052363008 net.cpp:247] Network initialization done.\n",
      "I0726 17:38:08.584205 2052363008 net.cpp:248] Memory required for data: 8086808\n",
      "I0726 17:38:08.584265 2052363008 solver.cpp:42] Solver scaffolding done.\n",
      "I0726 17:38:08.584297 2052363008 solver.cpp:250] Solving LeNet\n",
      "I0726 17:38:08.584301 2052363008 solver.cpp:251] Learning Rate Policy: inv\n",
      "I0726 17:38:08.584637 2052363008 solver.cpp:294] Iteration 0, Testing net (#0)\n",
      "I0726 17:38:09.299842 2052363008 solver.cpp:343]     Test net output #0: accuracy = 0.1014\n",
      "I0726 17:38:09.299870 2052363008 solver.cpp:343]     Test net output #1: loss = 2.34272 (* 1 = 2.34272 loss)\n",
      "I0726 17:38:09.305397 2052363008 solver.cpp:214] Iteration 0, loss = 2.36332\n",
      "I0726 17:38:09.305419 2052363008 solver.cpp:229]     Train net output #0: loss = 2.36332 (* 1 = 2.36332 loss)\n",
      "I0726 17:38:09.305436 2052363008 solver.cpp:486] Iteration 0, lr = 0.01\n",
      "I0726 17:38:10.572093 2052363008 solver.cpp:214] Iteration 100, loss = 0.21081\n",
      "I0726 17:38:10.572129 2052363008 solver.cpp:229]     Train net output #0: loss = 0.21081 (* 1 = 0.21081 loss)\n",
      "I0726 17:38:10.572135 2052363008 solver.cpp:486] Iteration 100, lr = 0.00992565\n",
      "I0726 17:38:11.752197 2052363008 solver.cpp:214] Iteration 200, loss = 0.136399\n",
      "I0726 17:38:11.752233 2052363008 solver.cpp:229]     Train net output #0: loss = 0.136399 (* 1 = 0.136399 loss)\n",
      "I0726 17:38:11.752240 2052363008 solver.cpp:486] Iteration 200, lr = 0.00985258\n",
      "I0726 17:38:12.935117 2052363008 solver.cpp:214] Iteration 300, loss = 0.159859\n",
      "I0726 17:38:12.935143 2052363008 solver.cpp:229]     Train net output #0: loss = 0.159859 (* 1 = 0.159859 loss)\n",
      "I0726 17:38:12.935150 2052363008 solver.cpp:486] Iteration 300, lr = 0.00978075\n",
      "I0726 17:38:14.120532 2052363008 solver.cpp:214] Iteration 400, loss = 0.067649\n",
      "I0726 17:38:14.120570 2052363008 solver.cpp:229]     Train net output #0: loss = 0.067649 (* 1 = 0.067649 loss)\n",
      "I0726 17:38:14.120576 2052363008 solver.cpp:486] Iteration 400, lr = 0.00971013\n",
      "I0726 17:38:15.317742 2052363008 solver.cpp:294] Iteration 500, Testing net (#0)\n",
      "I0726 17:38:15.867568 2052363008 solver.cpp:343]     Test net output #0: accuracy = 0.9738\n",
      "I0726 17:38:15.867614 2052363008 solver.cpp:343]     Test net output #1: loss = 0.0873849 (* 1 = 0.0873849 loss)\n",
      "I0726 17:38:15.871458 2052363008 solver.cpp:214] Iteration 500, loss = 0.0970333\n",
      "I0726 17:38:15.871491 2052363008 solver.cpp:229]     Train net output #0: loss = 0.0970332 (* 1 = 0.0970332 loss)\n",
      "I0726 17:38:15.871500 2052363008 solver.cpp:486] Iteration 500, lr = 0.00964069\n",
      "I0726 17:38:17.061460 2052363008 solver.cpp:214] Iteration 600, loss = 0.103316\n",
      "I0726 17:38:17.061493 2052363008 solver.cpp:229]     Train net output #0: loss = 0.103316 (* 1 = 0.103316 loss)\n",
      "I0726 17:38:17.061503 2052363008 solver.cpp:486] Iteration 600, lr = 0.0095724\n",
      "I0726 17:38:18.246060 2052363008 solver.cpp:214] Iteration 700, loss = 0.11961\n",
      "I0726 17:38:18.246098 2052363008 solver.cpp:229]     Train net output #0: loss = 0.11961 (* 1 = 0.11961 loss)\n",
      "I0726 17:38:18.246104 2052363008 solver.cpp:486] Iteration 700, lr = 0.00950522\n",
      "I0726 17:38:19.457228 2052363008 solver.cpp:214] Iteration 800, loss = 0.162436\n",
      "I0726 17:38:19.457267 2052363008 solver.cpp:229]     Train net output #0: loss = 0.162436 (* 1 = 0.162436 loss)\n",
      "I0726 17:38:19.457305 2052363008 solver.cpp:486] Iteration 800, lr = 0.00943913\n",
      "I0726 17:38:20.635257 2052363008 solver.cpp:214] Iteration 900, loss = 0.158127\n",
      "I0726 17:38:20.635303 2052363008 solver.cpp:229]     Train net output #0: loss = 0.158127 (* 1 = 0.158127 loss)\n",
      "I0726 17:38:20.635310 2052363008 solver.cpp:486] Iteration 900, lr = 0.00937411\n",
      "I0726 17:38:21.812459 2052363008 solver.cpp:294] Iteration 1000, Testing net (#0)\n",
      "I0726 17:38:22.351447 2052363008 solver.cpp:343]     Test net output #0: accuracy = 0.9812\n",
      "I0726 17:38:22.351475 2052363008 solver.cpp:343]     Test net output #1: loss = 0.0568733 (* 1 = 0.0568733 loss)\n",
      "I0726 17:38:22.355271 2052363008 solver.cpp:214] Iteration 1000, loss = 0.0836374\n",
      "I0726 17:38:22.355288 2052363008 solver.cpp:229]     Train net output #0: loss = 0.0836374 (* 1 = 0.0836374 loss)\n",
      "I0726 17:38:22.355295 2052363008 solver.cpp:486] Iteration 1000, lr = 0.00931012\n",
      "I0726 17:38:23.540537 2052363008 solver.cpp:214] Iteration 1100, loss = 0.00858144\n",
      "I0726 17:38:23.540575 2052363008 solver.cpp:229]     Train net output #0: loss = 0.00858147 (* 1 = 0.00858147 loss)\n",
      "I0726 17:38:23.540581 2052363008 solver.cpp:486] Iteration 1100, lr = 0.00924715\n",
      "I0726 17:38:24.726032 2052363008 solver.cpp:214] Iteration 1200, loss = 0.0150521\n",
      "I0726 17:38:24.726061 2052363008 solver.cpp:229]     Train net output #0: loss = 0.0150521 (* 1 = 0.0150521 loss)\n",
      "I0726 17:38:24.726068 2052363008 solver.cpp:486] Iteration 1200, lr = 0.00918515\n",
      "I0726 17:38:25.912447 2052363008 solver.cpp:214] Iteration 1300, loss = 0.0170034\n",
      "I0726 17:38:25.912473 2052363008 solver.cpp:229]     Train net output #0: loss = 0.0170034 (* 1 = 0.0170034 loss)\n",
      "I0726 17:38:25.912480 2052363008 solver.cpp:486] Iteration 1300, lr = 0.00912412\n",
      "I0726 17:38:27.104846 2052363008 solver.cpp:214] Iteration 1400, loss = 0.00753498\n",
      "I0726 17:38:27.104871 2052363008 solver.cpp:229]     Train net output #0: loss = 0.007535 (* 1 = 0.007535 loss)\n",
      "I0726 17:38:27.104877 2052363008 solver.cpp:486] Iteration 1400, lr = 0.00906403\n",
      "I0726 17:38:28.276499 2052363008 solver.cpp:294] Iteration 1500, Testing net (#0)\n",
      "I0726 17:38:28.810719 2052363008 solver.cpp:343]     Test net output #0: accuracy = 0.9846\n",
      "I0726 17:38:28.810749 2052363008 solver.cpp:343]     Test net output #1: loss = 0.0486546 (* 1 = 0.0486546 loss)\n",
      "I0726 17:38:28.815492 2052363008 solver.cpp:214] Iteration 1500, loss = 0.134859\n",
      "I0726 17:38:28.815510 2052363008 solver.cpp:229]     Train net output #0: loss = 0.134859 (* 1 = 0.134859 loss)\n",
      "I0726 17:38:28.815517 2052363008 solver.cpp:486] Iteration 1500, lr = 0.00900485\n",
      "I0726 17:38:29.998682 2052363008 solver.cpp:214] Iteration 1600, loss = 0.105947\n",
      "I0726 17:38:29.998711 2052363008 solver.cpp:229]     Train net output #0: loss = 0.105947 (* 1 = 0.105947 loss)\n",
      "I0726 17:38:29.998723 2052363008 solver.cpp:486] Iteration 1600, lr = 0.00894657\n",
      "I0726 17:38:31.189266 2052363008 solver.cpp:214] Iteration 1700, loss = 0.0169234\n",
      "I0726 17:38:31.189314 2052363008 solver.cpp:229]     Train net output #0: loss = 0.0169234 (* 1 = 0.0169234 loss)\n",
      "I0726 17:38:31.189321 2052363008 solver.cpp:486] Iteration 1700, lr = 0.00888916\n",
      "I0726 17:38:32.368914 2052363008 solver.cpp:214] Iteration 1800, loss = 0.018778\n",
      "I0726 17:38:32.368940 2052363008 solver.cpp:229]     Train net output #0: loss = 0.018778 (* 1 = 0.018778 loss)\n",
      "I0726 17:38:32.368947 2052363008 solver.cpp:486] Iteration 1800, lr = 0.0088326\n",
      "I0726 17:38:33.553004 2052363008 solver.cpp:214] Iteration 1900, loss = 0.128929\n",
      "I0726 17:38:33.553042 2052363008 solver.cpp:229]     Train net output #0: loss = 0.128929 (* 1 = 0.128929 loss)\n",
      "I0726 17:38:33.553061 2052363008 solver.cpp:486] Iteration 1900, lr = 0.00877687\n",
      "I0726 17:38:34.723800 2052363008 solver.cpp:294] Iteration 2000, Testing net (#0)\n",
      "I0726 17:38:35.256734 2052363008 solver.cpp:343]     Test net output #0: accuracy = 0.9862\n",
      "I0726 17:38:35.256762 2052363008 solver.cpp:343]     Test net output #1: loss = 0.0412676 (* 1 = 0.0412676 loss)\n",
      "I0726 17:38:35.260682 2052363008 solver.cpp:214] Iteration 2000, loss = 0.0207974\n",
      "I0726 17:38:35.260763 2052363008 solver.cpp:229]     Train net output #0: loss = 0.0207974 (* 1 = 0.0207974 loss)\n",
      "I0726 17:38:35.260778 2052363008 solver.cpp:486] Iteration 2000, lr = 0.00872196\n",
      "I0726 17:38:36.452476 2052363008 solver.cpp:214] Iteration 2100, loss = 0.02287\n",
      "I0726 17:38:36.452513 2052363008 solver.cpp:229]     Train net output #0: loss = 0.02287 (* 1 = 0.02287 loss)\n",
      "I0726 17:38:36.452520 2052363008 solver.cpp:486] Iteration 2100, lr = 0.00866784\n",
      "I0726 17:38:37.643887 2052363008 solver.cpp:214] Iteration 2200, loss = 0.0179842\n",
      "I0726 17:38:37.643928 2052363008 solver.cpp:229]     Train net output #0: loss = 0.0179842 (* 1 = 0.0179842 loss)\n",
      "I0726 17:38:37.643935 2052363008 solver.cpp:486] Iteration 2200, lr = 0.0086145\n",
      "I0726 17:38:38.830173 2052363008 solver.cpp:214] Iteration 2300, loss = 0.100966\n",
      "I0726 17:38:38.830224 2052363008 solver.cpp:229]     Train net output #0: loss = 0.100966 (* 1 = 0.100966 loss)\n",
      "I0726 17:38:38.830230 2052363008 solver.cpp:486] Iteration 2300, lr = 0.00856192\n",
      "I0726 17:38:40.016043 2052363008 solver.cpp:214] Iteration 2400, loss = 0.0132744\n",
      "I0726 17:38:40.016073 2052363008 solver.cpp:229]     Train net output #0: loss = 0.0132743 (* 1 = 0.0132743 loss)\n",
      "I0726 17:38:40.016080 2052363008 solver.cpp:486] Iteration 2400, lr = 0.00851008\n",
      "I0726 17:38:41.188561 2052363008 solver.cpp:294] Iteration 2500, Testing net (#0)\n",
      "I0726 17:38:41.721421 2052363008 solver.cpp:343]     Test net output #0: accuracy = 0.9854\n",
      "I0726 17:38:41.721448 2052363008 solver.cpp:343]     Test net output #1: loss = 0.0474931 (* 1 = 0.0474931 loss)\n",
      "I0726 17:38:41.725200 2052363008 solver.cpp:214] Iteration 2500, loss = 0.0361266\n",
      "I0726 17:38:41.725217 2052363008 solver.cpp:229]     Train net output #0: loss = 0.0361266 (* 1 = 0.0361266 loss)\n",
      "I0726 17:38:41.725224 2052363008 solver.cpp:486] Iteration 2500, lr = 0.00845897\n",
      "I0726 17:38:42.909611 2052363008 solver.cpp:214] Iteration 2600, loss = 0.0655455\n",
      "I0726 17:38:42.909649 2052363008 solver.cpp:229]     Train net output #0: loss = 0.0655455 (* 1 = 0.0655455 loss)\n",
      "I0726 17:38:42.909657 2052363008 solver.cpp:486] Iteration 2600, lr = 0.00840857\n",
      "I0726 17:38:44.092602 2052363008 solver.cpp:214] Iteration 2700, loss = 0.0573591\n",
      "I0726 17:38:44.092640 2052363008 solver.cpp:229]     Train net output #0: loss = 0.0573591 (* 1 = 0.0573591 loss)\n",
      "I0726 17:38:44.092648 2052363008 solver.cpp:486] Iteration 2700, lr = 0.00835886\n",
      "I0726 17:38:45.275074 2052363008 solver.cpp:214] Iteration 2800, loss = 0.00479946\n",
      "I0726 17:38:45.275111 2052363008 solver.cpp:229]     Train net output #0: loss = 0.00479937 (* 1 = 0.00479937 loss)\n",
      "I0726 17:38:45.275118 2052363008 solver.cpp:486] Iteration 2800, lr = 0.00830984\n",
      "I0726 17:38:46.463134 2052363008 solver.cpp:214] Iteration 2900, loss = 0.0269867\n",
      "I0726 17:38:46.463160 2052363008 solver.cpp:229]     Train net output #0: loss = 0.0269866 (* 1 = 0.0269866 loss)\n",
      "I0726 17:38:46.463167 2052363008 solver.cpp:486] Iteration 2900, lr = 0.00826148\n",
      "I0726 17:38:47.635181 2052363008 solver.cpp:294] Iteration 3000, Testing net (#0)\n",
      "I0726 17:38:48.168711 2052363008 solver.cpp:343]     Test net output #0: accuracy = 0.9864\n",
      "I0726 17:38:48.168747 2052363008 solver.cpp:343]     Test net output #1: loss = 0.0400642 (* 1 = 0.0400642 loss)\n",
      "I0726 17:38:48.172535 2052363008 solver.cpp:214] Iteration 3000, loss = 0.0159476\n",
      "I0726 17:38:48.172564 2052363008 solver.cpp:229]     Train net output #0: loss = 0.0159475 (* 1 = 0.0159475 loss)\n",
      "I0726 17:38:48.172572 2052363008 solver.cpp:486] Iteration 3000, lr = 0.00821377\n",
      "I0726 17:38:49.358373 2052363008 solver.cpp:214] Iteration 3100, loss = 0.00602336\n",
      "I0726 17:38:49.358400 2052363008 solver.cpp:229]     Train net output #0: loss = 0.00602325 (* 1 = 0.00602325 loss)\n",
      "I0726 17:38:49.358407 2052363008 solver.cpp:486] Iteration 3100, lr = 0.0081667\n",
      "I0726 17:38:50.540341 2052363008 solver.cpp:214] Iteration 3200, loss = 0.0100269\n",
      "I0726 17:38:50.540379 2052363008 solver.cpp:229]     Train net output #0: loss = 0.0100268 (* 1 = 0.0100268 loss)\n",
      "I0726 17:38:50.540386 2052363008 solver.cpp:486] Iteration 3200, lr = 0.00812025\n",
      "I0726 17:38:51.723223 2052363008 solver.cpp:214] Iteration 3300, loss = 0.0519916\n",
      "I0726 17:38:51.723261 2052363008 solver.cpp:229]     Train net output #0: loss = 0.0519915 (* 1 = 0.0519915 loss)\n",
      "I0726 17:38:51.723268 2052363008 solver.cpp:486] Iteration 3300, lr = 0.00807442\n",
      "I0726 17:38:52.904408 2052363008 solver.cpp:214] Iteration 3400, loss = 0.00966803\n",
      "I0726 17:38:52.904436 2052363008 solver.cpp:229]     Train net output #0: loss = 0.0096679 (* 1 = 0.0096679 loss)\n",
      "I0726 17:38:52.904443 2052363008 solver.cpp:486] Iteration 3400, lr = 0.00802918\n",
      "I0726 17:38:54.075974 2052363008 solver.cpp:294] Iteration 3500, Testing net (#0)\n",
      "I0726 17:38:54.608711 2052363008 solver.cpp:343]     Test net output #0: accuracy = 0.9881\n",
      "I0726 17:38:54.608750 2052363008 solver.cpp:343]     Test net output #1: loss = 0.0374038 (* 1 = 0.0374038 loss)\n",
      "I0726 17:38:54.612716 2052363008 solver.cpp:214] Iteration 3500, loss = 0.00936493\n",
      "I0726 17:38:54.612742 2052363008 solver.cpp:229]     Train net output #0: loss = 0.00936479 (* 1 = 0.00936479 loss)\n",
      "I0726 17:38:54.612751 2052363008 solver.cpp:486] Iteration 3500, lr = 0.00798454\n",
      "I0726 17:38:55.795058 2052363008 solver.cpp:214] Iteration 3600, loss = 0.0269274\n",
      "I0726 17:38:55.795094 2052363008 solver.cpp:229]     Train net output #0: loss = 0.0269273 (* 1 = 0.0269273 loss)\n",
      "I0726 17:38:55.795104 2052363008 solver.cpp:486] Iteration 3600, lr = 0.00794046\n",
      "I0726 17:38:56.976140 2052363008 solver.cpp:214] Iteration 3700, loss = 0.0175966\n",
      "I0726 17:38:56.976174 2052363008 solver.cpp:229]     Train net output #0: loss = 0.0175965 (* 1 = 0.0175965 loss)\n",
      "I0726 17:38:56.976187 2052363008 solver.cpp:486] Iteration 3700, lr = 0.00789695\n",
      "I0726 17:38:58.157526 2052363008 solver.cpp:214] Iteration 3800, loss = 0.0100966\n",
      "I0726 17:38:58.157555 2052363008 solver.cpp:229]     Train net output #0: loss = 0.0100965 (* 1 = 0.0100965 loss)\n",
      "I0726 17:38:58.157562 2052363008 solver.cpp:486] Iteration 3800, lr = 0.007854\n",
      "I0726 17:38:59.340608 2052363008 solver.cpp:214] Iteration 3900, loss = 0.0298793\n",
      "I0726 17:38:59.340638 2052363008 solver.cpp:229]     Train net output #0: loss = 0.0298792 (* 1 = 0.0298792 loss)\n",
      "I0726 17:38:59.340646 2052363008 solver.cpp:486] Iteration 3900, lr = 0.00781158\n",
      "I0726 17:39:00.512632 2052363008 solver.cpp:294] Iteration 4000, Testing net (#0)\n",
      "I0726 17:39:01.045433 2052363008 solver.cpp:343]     Test net output #0: accuracy = 0.99\n",
      "I0726 17:39:01.045469 2052363008 solver.cpp:343]     Test net output #1: loss = 0.0314633 (* 1 = 0.0314633 loss)\n",
      "I0726 17:39:01.049299 2052363008 solver.cpp:214] Iteration 4000, loss = 0.0165496\n",
      "I0726 17:39:01.049326 2052363008 solver.cpp:229]     Train net output #0: loss = 0.0165495 (* 1 = 0.0165495 loss)\n",
      "I0726 17:39:01.049334 2052363008 solver.cpp:486] Iteration 4000, lr = 0.0077697\n",
      "I0726 17:39:02.239681 2052363008 solver.cpp:214] Iteration 4100, loss = 0.0389241\n",
      "I0726 17:39:02.239718 2052363008 solver.cpp:229]     Train net output #0: loss = 0.038924 (* 1 = 0.038924 loss)\n",
      "I0726 17:39:02.239725 2052363008 solver.cpp:486] Iteration 4100, lr = 0.00772833\n",
      "I0726 17:39:03.422412 2052363008 solver.cpp:214] Iteration 4200, loss = 0.0102199\n",
      "I0726 17:39:03.422442 2052363008 solver.cpp:229]     Train net output #0: loss = 0.0102198 (* 1 = 0.0102198 loss)\n",
      "I0726 17:39:03.422449 2052363008 solver.cpp:486] Iteration 4200, lr = 0.00768748\n",
      "I0726 17:39:04.603406 2052363008 solver.cpp:214] Iteration 4300, loss = 0.0602281\n",
      "I0726 17:39:04.603435 2052363008 solver.cpp:229]     Train net output #0: loss = 0.060228 (* 1 = 0.060228 loss)\n",
      "I0726 17:39:04.603442 2052363008 solver.cpp:486] Iteration 4300, lr = 0.00764712\n",
      "I0726 17:39:05.781888 2052363008 solver.cpp:214] Iteration 4400, loss = 0.0225251\n",
      "I0726 17:39:05.781915 2052363008 solver.cpp:229]     Train net output #0: loss = 0.022525 (* 1 = 0.022525 loss)\n",
      "I0726 17:39:05.781924 2052363008 solver.cpp:486] Iteration 4400, lr = 0.00760726\n",
      "I0726 17:39:06.949578 2052363008 solver.cpp:294] Iteration 4500, Testing net (#0)\n",
      "I0726 17:39:07.481972 2052363008 solver.cpp:343]     Test net output #0: accuracy = 0.9881\n",
      "I0726 17:39:07.482002 2052363008 solver.cpp:343]     Test net output #1: loss = 0.0360985 (* 1 = 0.0360985 loss)\n",
      "I0726 17:39:07.485769 2052363008 solver.cpp:214] Iteration 4500, loss = 0.00848099\n",
      "I0726 17:39:07.485788 2052363008 solver.cpp:229]     Train net output #0: loss = 0.00848089 (* 1 = 0.00848089 loss)\n",
      "I0726 17:39:07.485796 2052363008 solver.cpp:486] Iteration 4500, lr = 0.00756788\n",
      "I0726 17:39:08.667444 2052363008 solver.cpp:214] Iteration 4600, loss = 0.0115927\n",
      "I0726 17:39:08.667475 2052363008 solver.cpp:229]     Train net output #0: loss = 0.0115926 (* 1 = 0.0115926 loss)\n",
      "I0726 17:39:08.667484 2052363008 solver.cpp:486] Iteration 4600, lr = 0.00752897\n",
      "I0726 17:39:09.850296 2052363008 solver.cpp:214] Iteration 4700, loss = 0.00480901\n",
      "I0726 17:39:09.850359 2052363008 solver.cpp:229]     Train net output #0: loss = 0.00480889 (* 1 = 0.00480889 loss)\n",
      "I0726 17:39:09.850368 2052363008 solver.cpp:486] Iteration 4700, lr = 0.00749052\n",
      "I0726 17:39:11.050092 2052363008 solver.cpp:214] Iteration 4800, loss = 0.0119218\n",
      "I0726 17:39:11.050122 2052363008 solver.cpp:229]     Train net output #0: loss = 0.0119217 (* 1 = 0.0119217 loss)\n",
      "I0726 17:39:11.050132 2052363008 solver.cpp:486] Iteration 4800, lr = 0.00745253\n",
      "I0726 17:39:12.235113 2052363008 solver.cpp:214] Iteration 4900, loss = 0.00600947\n",
      "I0726 17:39:12.235144 2052363008 solver.cpp:229]     Train net output #0: loss = 0.00600933 (* 1 = 0.00600933 loss)\n",
      "I0726 17:39:12.235152 2052363008 solver.cpp:486] Iteration 4900, lr = 0.00741498\n",
      "I0726 17:39:13.421397 2052363008 solver.cpp:361] Snapshotting to lenet_iter_5000.caffemodel\n",
      "I0726 17:39:13.432420 2052363008 solver.cpp:369] Snapshotting solver state to lenet_iter_5000.solverstate\n",
      "I0726 17:39:13.441148 2052363008 solver.cpp:294] Iteration 5000, Testing net (#0)\n",
      "I0726 17:39:13.967244 2052363008 solver.cpp:343]     Test net output #0: accuracy = 0.99\n",
      "I0726 17:39:13.967279 2052363008 solver.cpp:343]     Test net output #1: loss = 0.0308765 (* 1 = 0.0308765 loss)\n",
      "I0726 17:39:13.971086 2052363008 solver.cpp:214] Iteration 5000, loss = 0.0273059\n",
      "I0726 17:39:13.971108 2052363008 solver.cpp:229]     Train net output #0: loss = 0.0273058 (* 1 = 0.0273058 loss)\n",
      "I0726 17:39:13.971117 2052363008 solver.cpp:486] Iteration 5000, lr = 0.00737788\n",
      "I0726 17:39:15.153709 2052363008 solver.cpp:214] Iteration 5100, loss = 0.0149885\n",
      "I0726 17:39:15.153745 2052363008 solver.cpp:229]     Train net output #0: loss = 0.0149884 (* 1 = 0.0149884 loss)\n",
      "I0726 17:39:15.153754 2052363008 solver.cpp:486] Iteration 5100, lr = 0.0073412\n",
      "I0726 17:39:16.337740 2052363008 solver.cpp:214] Iteration 5200, loss = 0.00645413\n",
      "I0726 17:39:16.337772 2052363008 solver.cpp:229]     Train net output #0: loss = 0.00645397 (* 1 = 0.00645397 loss)\n",
      "I0726 17:39:16.337781 2052363008 solver.cpp:486] Iteration 5200, lr = 0.00730495\n",
      "I0726 17:39:17.517346 2052363008 solver.cpp:214] Iteration 5300, loss = 0.00370412\n",
      "I0726 17:39:17.517376 2052363008 solver.cpp:229]     Train net output #0: loss = 0.00370398 (* 1 = 0.00370398 loss)\n",
      "I0726 17:39:17.517385 2052363008 solver.cpp:486] Iteration 5300, lr = 0.00726911\n",
      "I0726 17:39:18.710907 2052363008 solver.cpp:214] Iteration 5400, loss = 0.0104691\n",
      "I0726 17:39:18.710937 2052363008 solver.cpp:229]     Train net output #0: loss = 0.010469 (* 1 = 0.010469 loss)\n",
      "I0726 17:39:18.710957 2052363008 solver.cpp:486] Iteration 5400, lr = 0.00723368\n",
      "I0726 17:39:19.895614 2052363008 solver.cpp:294] Iteration 5500, Testing net (#0)\n",
      "I0726 17:39:20.430649 2052363008 solver.cpp:343]     Test net output #0: accuracy = 0.989\n",
      "I0726 17:39:20.430681 2052363008 solver.cpp:343]     Test net output #1: loss = 0.0326148 (* 1 = 0.0326148 loss)\n",
      "I0726 17:39:20.434536 2052363008 solver.cpp:214] Iteration 5500, loss = 0.0083086\n",
      "I0726 17:39:20.434561 2052363008 solver.cpp:229]     Train net output #0: loss = 0.00830849 (* 1 = 0.00830849 loss)\n",
      "I0726 17:39:20.434571 2052363008 solver.cpp:486] Iteration 5500, lr = 0.00719865\n",
      "I0726 17:39:21.615819 2052363008 solver.cpp:214] Iteration 5600, loss = 0.00050589\n",
      "I0726 17:39:21.615852 2052363008 solver.cpp:229]     Train net output #0: loss = 0.000505788 (* 1 = 0.000505788 loss)\n",
      "I0726 17:39:21.615862 2052363008 solver.cpp:486] Iteration 5600, lr = 0.00716402\n",
      "I0726 17:39:22.796468 2052363008 solver.cpp:214] Iteration 5700, loss = 0.00382208\n",
      "I0726 17:39:22.796499 2052363008 solver.cpp:229]     Train net output #0: loss = 0.00382198 (* 1 = 0.00382198 loss)\n",
      "I0726 17:39:22.796509 2052363008 solver.cpp:486] Iteration 5700, lr = 0.00712977\n",
      "I0726 17:39:23.978842 2052363008 solver.cpp:214] Iteration 5800, loss = 0.0282736\n",
      "I0726 17:39:23.978873 2052363008 solver.cpp:229]     Train net output #0: loss = 0.0282735 (* 1 = 0.0282735 loss)\n",
      "I0726 17:39:23.978883 2052363008 solver.cpp:486] Iteration 5800, lr = 0.0070959\n",
      "I0726 17:39:25.160411 2052363008 solver.cpp:214] Iteration 5900, loss = 0.00644994\n",
      "I0726 17:39:25.160480 2052363008 solver.cpp:229]     Train net output #0: loss = 0.00644983 (* 1 = 0.00644983 loss)\n",
      "I0726 17:39:25.160488 2052363008 solver.cpp:486] Iteration 5900, lr = 0.0070624\n",
      "I0726 17:39:26.331372 2052363008 solver.cpp:294] Iteration 6000, Testing net (#0)\n",
      "I0726 17:39:26.865272 2052363008 solver.cpp:343]     Test net output #0: accuracy = 0.9915\n",
      "I0726 17:39:26.865304 2052363008 solver.cpp:343]     Test net output #1: loss = 0.0292589 (* 1 = 0.0292589 loss)\n",
      "I0726 17:39:26.869134 2052363008 solver.cpp:214] Iteration 6000, loss = 0.00268755\n",
      "I0726 17:39:26.869154 2052363008 solver.cpp:229]     Train net output #0: loss = 0.00268744 (* 1 = 0.00268744 loss)\n",
      "I0726 17:39:26.869163 2052363008 solver.cpp:486] Iteration 6000, lr = 0.00702927\n",
      "I0726 17:39:28.052063 2052363008 solver.cpp:214] Iteration 6100, loss = 0.00260057\n",
      "I0726 17:39:28.052095 2052363008 solver.cpp:229]     Train net output #0: loss = 0.00260046 (* 1 = 0.00260046 loss)\n",
      "I0726 17:39:28.052103 2052363008 solver.cpp:486] Iteration 6100, lr = 0.0069965\n",
      "I0726 17:39:29.231884 2052363008 solver.cpp:214] Iteration 6200, loss = 0.0121057\n",
      "I0726 17:39:29.231914 2052363008 solver.cpp:229]     Train net output #0: loss = 0.0121056 (* 1 = 0.0121056 loss)\n",
      "I0726 17:39:29.231922 2052363008 solver.cpp:486] Iteration 6200, lr = 0.00696408\n",
      "I0726 17:39:30.412272 2052363008 solver.cpp:214] Iteration 6300, loss = 0.0113306\n",
      "I0726 17:39:30.412302 2052363008 solver.cpp:229]     Train net output #0: loss = 0.0113305 (* 1 = 0.0113305 loss)\n",
      "I0726 17:39:30.412310 2052363008 solver.cpp:486] Iteration 6300, lr = 0.00693201\n",
      "I0726 17:39:31.591296 2052363008 solver.cpp:214] Iteration 6400, loss = 0.00655436\n",
      "I0726 17:39:31.591330 2052363008 solver.cpp:229]     Train net output #0: loss = 0.00655425 (* 1 = 0.00655425 loss)\n",
      "I0726 17:39:31.591337 2052363008 solver.cpp:486] Iteration 6400, lr = 0.00690029\n",
      "I0726 17:39:32.762972 2052363008 solver.cpp:294] Iteration 6500, Testing net (#0)\n",
      "I0726 17:39:33.296099 2052363008 solver.cpp:343]     Test net output #0: accuracy = 0.9915\n",
      "I0726 17:39:33.296129 2052363008 solver.cpp:343]     Test net output #1: loss = 0.0287745 (* 1 = 0.0287745 loss)\n",
      "I0726 17:39:33.299938 2052363008 solver.cpp:214] Iteration 6500, loss = 0.010065\n",
      "I0726 17:39:33.299959 2052363008 solver.cpp:229]     Train net output #0: loss = 0.0100649 (* 1 = 0.0100649 loss)\n",
      "I0726 17:39:33.299968 2052363008 solver.cpp:486] Iteration 6500, lr = 0.0068689\n",
      "I0726 17:39:34.480481 2052363008 solver.cpp:214] Iteration 6600, loss = 0.0322556\n",
      "I0726 17:39:34.480516 2052363008 solver.cpp:229]     Train net output #0: loss = 0.0322555 (* 1 = 0.0322555 loss)\n",
      "I0726 17:39:34.480526 2052363008 solver.cpp:486] Iteration 6600, lr = 0.00683784\n",
      "I0726 17:39:35.658541 2052363008 solver.cpp:214] Iteration 6700, loss = 0.00797286\n",
      "I0726 17:39:35.658571 2052363008 solver.cpp:229]     Train net output #0: loss = 0.00797274 (* 1 = 0.00797274 loss)\n",
      "I0726 17:39:35.658581 2052363008 solver.cpp:486] Iteration 6700, lr = 0.00680711\n",
      "I0726 17:39:36.838069 2052363008 solver.cpp:214] Iteration 6800, loss = 0.00273987\n",
      "I0726 17:39:36.838107 2052363008 solver.cpp:229]     Train net output #0: loss = 0.00273975 (* 1 = 0.00273975 loss)\n",
      "I0726 17:39:36.838121 2052363008 solver.cpp:486] Iteration 6800, lr = 0.0067767\n",
      "I0726 17:39:38.015668 2052363008 solver.cpp:214] Iteration 6900, loss = 0.00729337\n",
      "I0726 17:39:38.015698 2052363008 solver.cpp:229]     Train net output #0: loss = 0.00729326 (* 1 = 0.00729326 loss)\n",
      "I0726 17:39:38.015707 2052363008 solver.cpp:486] Iteration 6900, lr = 0.0067466\n",
      "I0726 17:39:39.187064 2052363008 solver.cpp:294] Iteration 7000, Testing net (#0)\n",
      "I0726 17:39:39.717824 2052363008 solver.cpp:343]     Test net output #0: accuracy = 0.9909\n",
      "I0726 17:39:39.717855 2052363008 solver.cpp:343]     Test net output #1: loss = 0.0282198 (* 1 = 0.0282198 loss)\n",
      "I0726 17:39:39.721627 2052363008 solver.cpp:214] Iteration 7000, loss = 0.00351764\n",
      "I0726 17:39:39.721647 2052363008 solver.cpp:229]     Train net output #0: loss = 0.00351752 (* 1 = 0.00351752 loss)\n",
      "I0726 17:39:39.721655 2052363008 solver.cpp:486] Iteration 7000, lr = 0.00671681\n",
      "I0726 17:39:40.897966 2052363008 solver.cpp:214] Iteration 7100, loss = 0.0171686\n",
      "I0726 17:39:40.898008 2052363008 solver.cpp:229]     Train net output #0: loss = 0.0171685 (* 1 = 0.0171685 loss)\n",
      "I0726 17:39:40.898016 2052363008 solver.cpp:486] Iteration 7100, lr = 0.00668733\n",
      "I0726 17:39:42.076447 2052363008 solver.cpp:214] Iteration 7200, loss = 0.00508261\n",
      "I0726 17:39:42.076477 2052363008 solver.cpp:229]     Train net output #0: loss = 0.0050825 (* 1 = 0.0050825 loss)\n",
      "I0726 17:39:42.076484 2052363008 solver.cpp:486] Iteration 7200, lr = 0.00665815\n",
      "I0726 17:39:43.282701 2052363008 solver.cpp:214] Iteration 7300, loss = 0.0212542\n",
      "I0726 17:39:43.282732 2052363008 solver.cpp:229]     Train net output #0: loss = 0.0212541 (* 1 = 0.0212541 loss)\n",
      "I0726 17:39:43.282738 2052363008 solver.cpp:486] Iteration 7300, lr = 0.00662927\n",
      "I0726 17:39:44.474562 2052363008 solver.cpp:214] Iteration 7400, loss = 0.00931773\n",
      "I0726 17:39:44.474591 2052363008 solver.cpp:229]     Train net output #0: loss = 0.00931763 (* 1 = 0.00931763 loss)\n",
      "I0726 17:39:44.474597 2052363008 solver.cpp:486] Iteration 7400, lr = 0.00660067\n",
      "I0726 17:39:45.640292 2052363008 solver.cpp:294] Iteration 7500, Testing net (#0)\n",
      "I0726 17:39:46.170971 2052363008 solver.cpp:343]     Test net output #0: accuracy = 0.9902\n",
      "I0726 17:39:46.171001 2052363008 solver.cpp:343]     Test net output #1: loss = 0.0319514 (* 1 = 0.0319514 loss)\n",
      "I0726 17:39:46.174743 2052363008 solver.cpp:214] Iteration 7500, loss = 0.00194153\n",
      "I0726 17:39:46.174762 2052363008 solver.cpp:229]     Train net output #0: loss = 0.00194143 (* 1 = 0.00194143 loss)\n",
      "I0726 17:39:46.174769 2052363008 solver.cpp:486] Iteration 7500, lr = 0.00657236\n",
      "I0726 17:39:47.353396 2052363008 solver.cpp:214] Iteration 7600, loss = 0.00217882\n",
      "I0726 17:39:47.353426 2052363008 solver.cpp:229]     Train net output #0: loss = 0.00217872 (* 1 = 0.00217872 loss)\n",
      "I0726 17:39:47.353433 2052363008 solver.cpp:486] Iteration 7600, lr = 0.00654433\n",
      "I0726 17:39:48.534629 2052363008 solver.cpp:214] Iteration 7700, loss = 0.0196372\n",
      "I0726 17:39:48.534662 2052363008 solver.cpp:229]     Train net output #0: loss = 0.0196371 (* 1 = 0.0196371 loss)\n",
      "I0726 17:39:48.534668 2052363008 solver.cpp:486] Iteration 7700, lr = 0.00651658\n",
      "I0726 17:39:49.710753 2052363008 solver.cpp:214] Iteration 7800, loss = 0.00353343\n",
      "I0726 17:39:49.710780 2052363008 solver.cpp:229]     Train net output #0: loss = 0.00353334 (* 1 = 0.00353334 loss)\n",
      "I0726 17:39:49.710788 2052363008 solver.cpp:486] Iteration 7800, lr = 0.00648911\n",
      "I0726 17:39:50.890820 2052363008 solver.cpp:214] Iteration 7900, loss = 0.00584857\n",
      "I0726 17:39:50.890849 2052363008 solver.cpp:229]     Train net output #0: loss = 0.00584848 (* 1 = 0.00584848 loss)\n",
      "I0726 17:39:50.890857 2052363008 solver.cpp:486] Iteration 7900, lr = 0.0064619\n",
      "I0726 17:39:52.055905 2052363008 solver.cpp:294] Iteration 8000, Testing net (#0)\n",
      "I0726 17:39:52.585865 2052363008 solver.cpp:343]     Test net output #0: accuracy = 0.9916\n",
      "I0726 17:39:52.585891 2052363008 solver.cpp:343]     Test net output #1: loss = 0.0282878 (* 1 = 0.0282878 loss)\n",
      "I0726 17:39:52.589548 2052363008 solver.cpp:214] Iteration 8000, loss = 0.0073061\n",
      "I0726 17:39:52.589565 2052363008 solver.cpp:229]     Train net output #0: loss = 0.007306 (* 1 = 0.007306 loss)\n",
      "I0726 17:39:52.589572 2052363008 solver.cpp:486] Iteration 8000, lr = 0.00643496\n",
      "I0726 17:39:53.764652 2052363008 solver.cpp:214] Iteration 8100, loss = 0.0147352\n",
      "I0726 17:39:53.764681 2052363008 solver.cpp:229]     Train net output #0: loss = 0.0147351 (* 1 = 0.0147351 loss)\n",
      "I0726 17:39:53.764689 2052363008 solver.cpp:486] Iteration 8100, lr = 0.00640827\n",
      "I0726 17:39:54.941072 2052363008 solver.cpp:214] Iteration 8200, loss = 0.00751953\n",
      "I0726 17:39:54.941100 2052363008 solver.cpp:229]     Train net output #0: loss = 0.00751943 (* 1 = 0.00751943 loss)\n",
      "I0726 17:39:54.941107 2052363008 solver.cpp:486] Iteration 8200, lr = 0.00638185\n",
      "I0726 17:39:56.116767 2052363008 solver.cpp:214] Iteration 8300, loss = 0.0438099\n",
      "I0726 17:39:56.116797 2052363008 solver.cpp:229]     Train net output #0: loss = 0.0438098 (* 1 = 0.0438098 loss)\n",
      "I0726 17:39:56.116806 2052363008 solver.cpp:486] Iteration 8300, lr = 0.00635567\n",
      "I0726 17:39:57.298398 2052363008 solver.cpp:214] Iteration 8400, loss = 0.00806795\n",
      "I0726 17:39:57.298429 2052363008 solver.cpp:229]     Train net output #0: loss = 0.00806785 (* 1 = 0.00806785 loss)\n",
      "I0726 17:39:57.298435 2052363008 solver.cpp:486] Iteration 8400, lr = 0.00632975\n",
      "I0726 17:39:58.469187 2052363008 solver.cpp:294] Iteration 8500, Testing net (#0)\n",
      "I0726 17:39:59.002312 2052363008 solver.cpp:343]     Test net output #0: accuracy = 0.9915\n",
      "I0726 17:39:59.002341 2052363008 solver.cpp:343]     Test net output #1: loss = 0.0289984 (* 1 = 0.0289984 loss)\n",
      "I0726 17:39:59.006067 2052363008 solver.cpp:214] Iteration 8500, loss = 0.00839201\n",
      "I0726 17:39:59.006084 2052363008 solver.cpp:229]     Train net output #0: loss = 0.00839191 (* 1 = 0.00839191 loss)\n",
      "I0726 17:39:59.006091 2052363008 solver.cpp:486] Iteration 8500, lr = 0.00630407\n",
      "I0726 17:40:00.190428 2052363008 solver.cpp:214] Iteration 8600, loss = 0.000726519\n",
      "I0726 17:40:00.190456 2052363008 solver.cpp:229]     Train net output #0: loss = 0.000726421 (* 1 = 0.000726421 loss)\n",
      "I0726 17:40:00.190464 2052363008 solver.cpp:486] Iteration 8600, lr = 0.00627864\n",
      "I0726 17:40:01.375983 2052363008 solver.cpp:214] Iteration 8700, loss = 0.00291301\n",
      "I0726 17:40:01.376011 2052363008 solver.cpp:229]     Train net output #0: loss = 0.00291291 (* 1 = 0.00291291 loss)\n",
      "I0726 17:40:01.376019 2052363008 solver.cpp:486] Iteration 8700, lr = 0.00625344\n",
      "I0726 17:40:02.559312 2052363008 solver.cpp:214] Iteration 8800, loss = 0.00117272\n",
      "I0726 17:40:02.559340 2052363008 solver.cpp:229]     Train net output #0: loss = 0.00117262 (* 1 = 0.00117262 loss)\n",
      "I0726 17:40:02.559346 2052363008 solver.cpp:486] Iteration 8800, lr = 0.00622847\n",
      "I0726 17:40:03.740816 2052363008 solver.cpp:214] Iteration 8900, loss = 0.00196562\n",
      "I0726 17:40:03.740856 2052363008 solver.cpp:229]     Train net output #0: loss = 0.00196552 (* 1 = 0.00196552 loss)\n",
      "I0726 17:40:03.740864 2052363008 solver.cpp:486] Iteration 8900, lr = 0.00620374\n",
      "I0726 17:40:04.909484 2052363008 solver.cpp:294] Iteration 9000, Testing net (#0)\n",
      "I0726 17:40:05.443176 2052363008 solver.cpp:343]     Test net output #0: accuracy = 0.9909\n",
      "I0726 17:40:05.443203 2052363008 solver.cpp:343]     Test net output #1: loss = 0.0290432 (* 1 = 0.0290432 loss)\n",
      "I0726 17:40:05.447013 2052363008 solver.cpp:214] Iteration 9000, loss = 0.0162694\n",
      "I0726 17:40:05.447031 2052363008 solver.cpp:229]     Train net output #0: loss = 0.0162693 (* 1 = 0.0162693 loss)\n",
      "I0726 17:40:05.447037 2052363008 solver.cpp:486] Iteration 9000, lr = 0.00617924\n",
      "I0726 17:40:06.627403 2052363008 solver.cpp:214] Iteration 9100, loss = 0.00942984\n",
      "I0726 17:40:06.627435 2052363008 solver.cpp:229]     Train net output #0: loss = 0.00942973 (* 1 = 0.00942973 loss)\n",
      "I0726 17:40:06.627449 2052363008 solver.cpp:486] Iteration 9100, lr = 0.00615496\n",
      "I0726 17:40:07.808423 2052363008 solver.cpp:214] Iteration 9200, loss = 0.00223762\n",
      "I0726 17:40:07.808460 2052363008 solver.cpp:229]     Train net output #0: loss = 0.00223751 (* 1 = 0.00223751 loss)\n",
      "I0726 17:40:07.808478 2052363008 solver.cpp:486] Iteration 9200, lr = 0.0061309\n",
      "I0726 17:40:08.992595 2052363008 solver.cpp:214] Iteration 9300, loss = 0.00958583\n",
      "I0726 17:40:08.992631 2052363008 solver.cpp:229]     Train net output #0: loss = 0.00958572 (* 1 = 0.00958572 loss)\n",
      "I0726 17:40:08.992641 2052363008 solver.cpp:486] Iteration 9300, lr = 0.00610706\n",
      "I0726 17:40:10.176336 2052363008 solver.cpp:214] Iteration 9400, loss = 0.0308418\n",
      "I0726 17:40:10.176373 2052363008 solver.cpp:229]     Train net output #0: loss = 0.0308417 (* 1 = 0.0308417 loss)\n",
      "I0726 17:40:10.176381 2052363008 solver.cpp:486] Iteration 9400, lr = 0.00608343\n",
      "I0726 17:40:11.348728 2052363008 solver.cpp:294] Iteration 9500, Testing net (#0)\n",
      "I0726 17:40:11.883316 2052363008 solver.cpp:343]     Test net output #0: accuracy = 0.9897\n",
      "I0726 17:40:11.883343 2052363008 solver.cpp:343]     Test net output #1: loss = 0.0338839 (* 1 = 0.0338839 loss)\n",
      "I0726 17:40:11.887110 2052363008 solver.cpp:214] Iteration 9500, loss = 0.00395738\n",
      "I0726 17:40:11.887126 2052363008 solver.cpp:229]     Train net output #0: loss = 0.00395727 (* 1 = 0.00395727 loss)\n",
      "I0726 17:40:11.887133 2052363008 solver.cpp:486] Iteration 9500, lr = 0.00606002\n",
      "I0726 17:40:13.067878 2052363008 solver.cpp:214] Iteration 9600, loss = 0.00414806\n",
      "I0726 17:40:13.067917 2052363008 solver.cpp:229]     Train net output #0: loss = 0.00414795 (* 1 = 0.00414795 loss)\n",
      "I0726 17:40:13.067924 2052363008 solver.cpp:486] Iteration 9600, lr = 0.00603682\n",
      "I0726 17:40:14.251572 2052363008 solver.cpp:214] Iteration 9700, loss = 0.00441311\n",
      "I0726 17:40:14.251613 2052363008 solver.cpp:229]     Train net output #0: loss = 0.004413 (* 1 = 0.004413 loss)\n",
      "I0726 17:40:14.251621 2052363008 solver.cpp:486] Iteration 9700, lr = 0.00601382\n",
      "I0726 17:40:15.435796 2052363008 solver.cpp:214] Iteration 9800, loss = 0.0105979\n",
      "I0726 17:40:15.435823 2052363008 solver.cpp:229]     Train net output #0: loss = 0.0105978 (* 1 = 0.0105978 loss)\n",
      "I0726 17:40:15.435830 2052363008 solver.cpp:486] Iteration 9800, lr = 0.00599102\n",
      "I0726 17:40:16.617491 2052363008 solver.cpp:214] Iteration 9900, loss = 0.00372694\n",
      "I0726 17:40:16.617528 2052363008 solver.cpp:229]     Train net output #0: loss = 0.00372684 (* 1 = 0.00372684 loss)\n",
      "I0726 17:40:16.617535 2052363008 solver.cpp:486] Iteration 9900, lr = 0.00596843\n",
      "I0726 17:40:17.797776 2052363008 solver.cpp:361] Snapshotting to lenet_iter_10000.caffemodel\n",
      "I0726 17:40:17.805953 2052363008 solver.cpp:369] Snapshotting solver state to lenet_iter_10000.solverstate\n",
      "I0726 17:40:17.817582 2052363008 solver.cpp:276] Iteration 10000, loss = 0.00354223\n",
      "I0726 17:40:17.817608 2052363008 solver.cpp:294] Iteration 10000, Testing net (#0)\n",
      "I0726 17:40:18.344795 2052363008 solver.cpp:343]     Test net output #0: accuracy = 0.9917\n",
      "I0726 17:40:18.344820 2052363008 solver.cpp:343]     Test net output #1: loss = 0.0278814 (* 1 = 0.0278814 loss)\n",
      "I0726 17:40:18.344827 2052363008 solver.cpp:281] Optimization Done.\n",
      "I0726 17:40:18.344831 2052363008 caffe.cpp:134] Optimization Done.\n",
      "\n",
      "real\t2m10.132s\n",
      "user\t2m12.507s\n",
      "sys\t0m2.633s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "  \n",
    "time bash train_lenet.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Check performance\n",
    "nvidia-smi\n",
    "istats\n",
    "\n",
    "# Check device configuration\n",
    "deviceQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Accelerated Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Libraries\n",
    "Compiler Directives\n",
    "CUDA or Open-CL enabled language"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Libraries\n",
    "Built to optimize GPU performance\n",
    "Requires minimal code change to switch between CPU and GPU"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Compiler Directives\n",
    "Use explicit compiler commands for tasks like: \n",
    "    Explain how to move data between chips\n",
    "    Identify code blocks to accelerate and enable parallelism\n",
    "    Tell it when to wait for work to complete\n",
    "    \n",
    "Example option OpenACC for languages like Fortran, C and C++ for directives\n",
    "OR @vectorize from NumbaPro for Python"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# CUDA enabled language\n",
    "Numba has decorators to use in Python scrip to help support using GPU\n",
    "\n",
    "@cuda.autojit - this decorator is used to tell the CUDA compiler that the function is to be compiled for the GPU. With autojit, the compiler will try and determine the type information of the variables being passed in. You can create your own signatures manually by using the jit decorator.\n",
    "cuda.blockIdx.x - this is a read-only variable that is defined for you. It is used within a GPU kernel to determine the ID of the block which is currently executing code. Since there will be many blocks running in parallel, we need this ID to help determine which chunk of data that particular block will work on.\n",
    "cuda.threadIdx.x - this is a read-only variable that is defined for you. It is used within a GPU kernel to determine the ID of the thread which is currently executing code in the active block.\n",
    "myKernel[number_of_blocks, threads_per_block](...) - this is the syntax used to launch a kernel on the GPU. Inside the list (the square brackets [...]), the first number is the total number of blocks we want to run on the GPU, and the second is the number of threads there are per block. It's possible, and in fact recommended, for one to schedule more blocks than the GPU can actively run in parallel. The system will just continue executing blocks until they have all completed. The following video addresses grids, blocks, and threads in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
